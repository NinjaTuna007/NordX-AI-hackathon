{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# set up for FAISS similarity search\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# set up the notebook\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "# import embeddings that are not OpenAI\n",
    "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# import RedisVectorStore\n",
    "# from langchain.vectorstores.redis import RedisVectorStore\n",
    "\n",
    "if True:\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(\"https://en.m.wikipedia.org/wiki/List_of_The_Looney_Tunes_Show_episodes#Season_1_(2011%E2%80%9312)\",),\n",
    "        bs_kwargs=dict(\n",
    "            # features=\"html.parser\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "else:\n",
    "    # use requests to get the html\n",
    "    url = \"https://en.m.wikipedia.org/wiki/List_of_The_Looney_Tunes_Show_episodes#Season_1_(2011%E2%80%9312)\"\n",
    "    response = requests.get(url)\n",
    "    # print(response.status_code)\n",
    "\n",
    "    # use beautiful soup to parse the html\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    # find paragraphs\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "    # find all the text\n",
    "    text = []\n",
    "    for paragraph in paragraphs:\n",
    "        text.append(paragraph.text)\n",
    "\n",
    "    # print(text)\n",
    "    \n",
    "    # remove the first paragraph\n",
    "    text = text[1:]\n",
    "\n",
    "    # remove the last paragraph\n",
    "    text = text[:-1]\n",
    "\n",
    "    # convert to a single object of class Document\n",
    "    docs = []\n",
    "    for i in range(len(text)):\n",
    "        docs.append(hub.Document(text[i]))    \n",
    "\n",
    "# print(docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# give new titles to the splits\n",
    "for i in range(len(splits)):\n",
    "    splits[i].metadata['title'] = \"Chunk {}\".format(i)\n",
    "\n",
    "# print (splits[-1])\n",
    "\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "# define a vectorstore without OpenAIEmbeddings\n",
    "if False:\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=FastEmbedEmbeddings())\n",
    "else:\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=FastEmbedEmbeddings(model_name=\"intfloat/multilingual-e5-large\", doc_embed_type=\"passage\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get the top n results with similarity scores\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores.redis import RedisVectorStoreRetriever\n",
    "\n",
    "\n",
    "class VectorStoreRetrieverWithScores(FAISS): # original Chroma | FAISS\n",
    "    \"\"\"\n",
    "    Hacky way to create a retriever that adds the score to the metadata\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vectorstore: FAISS, k: int = 10, search_type: str = \"similarity\"):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "        self.search_type = \"similarity\"\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # [NOTE] we removed the search type, only use search_type = \"similarity\"\n",
    "        if self.search_type != \"similarity\":\n",
    "            raise ValueError(f\"Only search_type='similarity' is supported with scores\")\n",
    "        # docs_and_scores = self.vectorstore.similarity_search_with_score(query, k=self.k)\n",
    "        docs_and_scores = self.vectorstore.similarity_search_with_relevance_scores(query, k=self.k)\n",
    "        # sort by score: highest first\n",
    "        docs_and_scores = sorted(docs_and_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        for doc, score in docs_and_scores:\n",
    "            doc.metadata = {**doc.metadata, **{\"score\": score}}\n",
    "        return [doc for (doc, _) in docs_and_scores]\n",
    "\n",
    "# define retriever\n",
    "retriever = VectorStoreRetrieverWithScores(vectorstore=vectorstore, k=5, search_type=\"similarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in query: 16\n",
      "Number of words in Modified query: 516\n",
      "Chunk 77 0.7630005790233896\n",
      "Chunk 47 0.7622321366079148\n",
      "Chunk 62 0.7613559247007975\n",
      "Chunk 74 0.7597999052006238\n",
      "Chunk 7 0.7583364401797801\n"
     ]
    }
   ],
   "source": [
    "# from langchain.retrievers import WikipediaRetriever\n",
    "\n",
    "# define a retriever\n",
    "# retriever = vectorstore.as_retriever(search_type='similarity_score_threshold',search_kwargs={\"k\":6, \"score_threshold\": 0.0})\n",
    "\n",
    "# define a query\n",
    "query = \"query: After watching a movie together, Bugs explains to Lola what a Dear John letter is.\"\n",
    "\n",
    "print(\"Number of words in query: {}\".format(len(query.split())))\n",
    "\n",
    "# random unicode text\n",
    "# rand_text_noise = \"\".join(np.random.choice(list(\"abcdefghijklmnopqrstuvw x y z \\n \"), size=3000000))\n",
    "\n",
    "if False:\n",
    "    # use lorem ipsum text generator\n",
    "    import lorem\n",
    "    # define random noise of specified length\n",
    "    desired_length = 512 # words\n",
    "    rand_text_noise = \"\"\n",
    "    while len(rand_text_noise.split()) < desired_length:\n",
    "        rand_text_noise += \" \" + lorem.sentence()\n",
    "        # print(len(rand_text_noise.split()))\n",
    "    print(\"Number of words in random noise: {}\".format(len(rand_text_noise.split())))\n",
    "    print(\"Random noise: {}\".format(rand_text_noise))\n",
    "\n",
    "else:\n",
    "    # TODO: Keywords - Query/Context tokens at beginning of query/context resp\n",
    "\n",
    "    # define article to take random noise from\n",
    "    # article = \"https://en.wikipedia.org/wiki/Looney_Tunes\"\n",
    "\n",
    "    # use an unrelated[orthogonal] article\n",
    "    article = \"https://en.wikipedia.org/wiki/Quantum_mechanics\"\n",
    "\n",
    "    if True: # collapsing the code for visibility\n",
    "        # get article\n",
    "        response = requests.get(article)\n",
    "        # parse article\n",
    "        soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "        # get all paragraphs\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        # get all text\n",
    "        text = [paragraph.text for paragraph in paragraphs]\n",
    "        # join all text\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # define random noise of specified length\n",
    "    desired_length = 500 # words\n",
    "\n",
    "    if True: # collapsing the code for visibility\n",
    "        rand_text_noise = []\n",
    "        while len(rand_text_noise) < desired_length:\n",
    "            rand_text_noise.append(np.random.choice(text.split()))\n",
    "            # print(len(rand_text_noise.split()))\n",
    "        rand_text_noise = \" \".join(rand_text_noise)\n",
    "\n",
    "        # print(\"Number of words in random noise: {}\".format(len(rand_text_noise.split())))\n",
    "        # print(\"Random noise: {}\".format(rand_text_noise))\n",
    "\n",
    "# add to end of query to make it longer\n",
    "query = query + \" \" + rand_text_noise\n",
    "# add to beginning of query to make it longer\n",
    "# query = rand_text_noise + query\n",
    "# magnify the query by repeating it\n",
    "# query = query * 25\n",
    "\n",
    "print(\"Number of words in Modified query: {}\".format(len(query.split())))\n",
    "\n",
    "# retrieve the results\n",
    "results = retriever.get_relevant_documents(query=query)\n",
    "\n",
    "if True: # more collapsing useless stuff\n",
    "    for result in results:\n",
    "        # print(dir(result))\n",
    "        print(result.metadata['title'], result.metadata['score'])\n",
    "\n",
    "    # for result in results:\n",
    "    #     print(result.page_content)\n",
    "    #     print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: where is the breakpoint for FastText Embedding.\n",
    "# Answer: 512 tokens\n",
    "\n",
    "#TODO: Embedding models\n",
    "# www.huggingface.co/intfloat/multilingual-e5-small\n",
    "# www.huggingface.co/intfloat/multilingual-e5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores.redis import Redis\n",
    "\n",
    "# rds = Redis.from_documents(\n",
    "#     documents=splits, \n",
    "#     embedding=FastEmbedEmbeddings(), \n",
    "#     redis_url = \"redis://localhost:6379\",\n",
    "#     index_name = \"looneytunes\",\n",
    "# )\n",
    "\n",
    "# retriever = RedisVectorStoreRetrieverWithScores(\n",
    "#     vectorstore=rds,\n",
    "#     search_type=\"similarity\",\n",
    "#     k=5,\n",
    "# )\n",
    "\n",
    "# deploy redis using docker\n",
    "# docker run -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "# kill the above docker container\n",
    "# docker kill $(docker ps -q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NEW APPROACH!\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torch import Tensor\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# def average_pool(last_hidden_states: Tensor,\n",
    "#                  attention_mask: Tensor) -> Tensor:\n",
    "#     last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "#     return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "# # Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n",
    "# # For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "# input_texts = ['query: how much protein should a female eat',\n",
    "#                'query: 南瓜的家常做法',\n",
    "#                \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "#                \"passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"]\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "# model = AutoModel.from_pretrained('intfloat/multilingual-e5-large')\n",
    "\n",
    "# # Tokenize the input texts\n",
    "# batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# outputs = model(**batch_dict)\n",
    "# embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "# print(embeddings.shape)\n",
    "\n",
    "# # normalize embeddings\n",
    "# embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "# scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "# print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adapt the above code to use the retriever\n",
    "\n",
    "# # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "# # splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # # give new titles to the splits\n",
    "# # for i in range(len(splits)):\n",
    "# #     splits[i].metadata['title'] = \"Chunk {}\".format(i)\n",
    "\n",
    "# from typing import List\n",
    "# from langchain.schema import Document\n",
    "# from langchain_core.embeddings import Embeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "# # define custom embedding model based on the above code\n",
    "# class CustomEmbeddingModel(Embeddings):\n",
    "#     def __init__(self, model_name: str = \"intfloat/multilingual-e5-large\"):\n",
    "#         self.model_name = model_name\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "#         self.model = AutoModel.from_pretrained(self.model_name)\n",
    "\n",
    "#     def embed(self, documents: List[Document]) -> List[Document]:\n",
    "#         # get the text from the documents\n",
    "#         texts = [document.text for document in documents]\n",
    "#         # tokenize the text\n",
    "#         batch_dict = self.tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "#         # get the embeddings\n",
    "#         outputs = self.model(**batch_dict)\n",
    "#         embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "#         # normalize the embeddings\n",
    "#         embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "#         # add the embeddings to the documents\n",
    "#         for i in range(len(documents)):\n",
    "#             documents[i].embedding = embeddings[i].tolist()\n",
    "#         return documents\n",
    "\n",
    "\n",
    "# # define a working vectorstore using the multi-lingual model\n",
    "# vectorstore = FAISS.from_documents(documents=splits, embedding=CustomEmbeddingModel())\n",
    "\n",
    "# # define a working retriever\n",
    "# retriever = VectorStoreRetrieverWithScores(vectorstore=vectorstore, k=5, search_type=\"similarity\")\n",
    "\n",
    "# # retrieve the results\n",
    "# results = retriever.get_relevant_documents(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
